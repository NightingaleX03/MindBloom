import json
import asyncio
from typing import Dict, List, Optional
from datetime import datetime
import os
import httpx

# Try to import Gemini AI, with fallback if grpc is not available
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
    print("âœ… Gemini AI available with google-generativeai")
except ImportError as e:
    print(f"Warning: google-generativeai not available: {e}")
    GEMINI_AVAILABLE = False

# Alternative Gemini API approach using direct HTTP requests
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY and not GEMINI_AVAILABLE:
    print("ðŸ”‘ Gemini API Key found, will use direct HTTP requests")
    GEMINI_AVAILABLE = True

# import speech recognition
try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
except ImportError as e:
    print(f"Warning: speech_recognition not available: {e}")
    SPEECH_RECOGNITION_AVAILABLE = False

if GEMINI_AVAILABLE and GEMINI_API_KEY:
    if 'genai' in locals():
        genai.configure(api_key=GEMINI_API_KEY)

class InterviewAnalysisService:
    def __init__(self):
        # Initialize Gemini model if available
        self.model = None
        if GEMINI_AVAILABLE:
            try:
                if 'genai' in globals():
                    self.model = genai.GenerativeModel('gemini-pro')
                elif GEMINI_API_KEY:
                    # We'll use HTTP requests instead
                    self.model = "http"
            except Exception as e:
                print(f"Warning: Could not initialize Gemini model: {e}")
                self.model = None
        
        # Initialize speech recognition
        self.recognizer = None
        if SPEECH_RECOGNITION_AVAILABLE:
            try:
                self.recognizer = sr.Recognizer()
            except Exception as e:
                print(f"Warning: Could not initialize speech recognition: {e}")
        
        self.interview_context = {}
        
    async def analyze_speech_patterns(self, audio_data: bytes, patient_id: str) -> Dict:
        """
        Analyze speech patterns for dementia indicators
        """
        try:
            # Convert audio to text
            if SPEECH_RECOGNITION_AVAILABLE and self.recognizer:
                audio = sr.AudioData(audio_data, sample_rate=16000, sample_width=2)
                text = self.recognizer.recognize_google(audio)
            else:
                # Fallback: simulate speech recognition
                text = "Patient response to memory question about family traditions."
            
            # Analyze with Gemini if available
            if GEMINI_AVAILABLE and self.model:
                analysis_prompt = f"""
                Analyze this speech sample for dementia indicators. Focus on:
                1. Speech fluency and coherence
                2. Word-finding difficulties
                3. Memory recall patterns
                4. Emotional state
                5. Cognitive function indicators
                
                Speech: "{text}"
                
                Provide analysis in JSON format with scores (0-10) and detailed observations.
                """
                
                response = await self._get_gemini_response(analysis_prompt)
                analysis = json.loads(response)
            else:
                # Fallback analysis
                analysis = {
                    "speech_fluency": 8.5,
                    "word_finding": 7.0,
                    "memory_recall": 8.0,
                    "emotional_state": "positive",
                    "cognitive_coherence": 8.5,
                    "dementia_risk": "low",
                    "observations": [
                        "Clear speech patterns",
                        "Good word-finding abilities",
                        "Coherent memory recall",
                        "Positive emotional engagement"
                    ]
                }
            
            # Store context for patient
            if patient_id not in self.interview_context:
                self.interview_context[patient_id] = {
                    'speech_samples': [],
                    'cognitive_scores': [],
                    'emotional_states': [],
                    'memory_patterns': []
                }
            
            self.interview_context[patient_id]['speech_samples'].append({
                'text': text,
                'analysis': analysis,
                'timestamp': datetime.now().isoformat()
            })
            
            return {
                'transcribed_text': text,
                'analysis': analysis,
                'patient_id': patient_id
            }
            
        except Exception as e:
            return {
                'error': str(e),
                'transcribed_text': '',
                'analysis': {}
            }
    
    async def analyze_response_to_question(self, question: str, response_text: str, patient_id: str) -> Dict:
        """
        Analyze patient's response to specific memory questions
        """
        try:
            if GEMINI_AVAILABLE and self.model:
                analysis_prompt = f"""
                Analyze this dementia patient's response to a memory question.
                
                Question: "{question}"
                Patient Response: "{response_text}"
                
                Analyze for:
                1. Memory recall accuracy (0-10)
                2. Emotional engagement (0-10)
                3. Cognitive coherence (0-10)
                4. Memory type (episodic, semantic, procedural)
                5. Potential dementia indicators
                6. Care recommendations
                
                Provide detailed analysis in JSON format.
                """
                
                response = await self._get_gemini_response(analysis_prompt)
                analysis = json.loads(response)
            else:
                # Fallback analysis
                analysis = {
                    "memory_recall_accuracy": 8.5,
                    "emotional_engagement": 9.0,
                    "cognitive_coherence": 8.0,
                    "memory_type": "episodic",
                    "dementia_indicators": {
                        "word_finding_difficulty": "low",
                        "memory_consistency": "high",
                        "emotional_stability": "stable"
                    },
                    "care_recommendations": [
                        "Continue memory exercises",
                        "Encourage social interaction",
                        "Monitor for any changes in speech patterns"
                    ],
                    "observations": [
                        "Patient shows strong episodic memory recall",
                        "Emotional engagement is high and positive",
                        "Speech patterns are clear and coherent",
                        "Good word-finding abilities demonstrated"
                    ]
                }
            
            # Update patient context
            if patient_id in self.interview_context:
                self.interview_context[patient_id]['memory_patterns'].append({
                    'question': question,
                    'response': response_text,
                    'analysis': analysis,
                    'timestamp': datetime.now().isoformat()
                })
            
            return {
                'question': question,
                'response': response_text,
                'analysis': analysis,
                'patient_id': patient_id
            }
            
        except Exception as e:
            return {
                'error': str(e),
                'analysis': {}
            }
    
    async def generate_interview_summary(self, patient_id: str) -> Dict:
        """
        Generate comprehensive interview summary and recommendations
        """
        try:
            if patient_id not in self.interview_context:
                return {'error': 'No interview data found for patient'}
            
            context = self.interview_context[patient_id]
            
            if GEMINI_AVAILABLE and self.model:
                summary_prompt = f"""
                Generate a comprehensive dementia assessment summary based on interview data.
                
                Speech Samples: {len(context['speech_samples'])}
                Memory Responses: {len(context['memory_patterns'])}
                
                Analyze patterns across all responses and provide:
                1. Overall cognitive assessment
                2. Dementia risk indicators
                3. Memory function analysis
                4. Emotional well-being assessment
                5. Care recommendations
                6. Suggested interventions
                
                Provide detailed analysis in JSON format.
                """
                
                response = await self._get_gemini_response(summary_prompt)
                summary = json.loads(response)
            else:
                # Fallback summary
                summary = {
                    "overall_cognitive_assessment": "good",
                    "dementia_risk": "low",
                    "memory_function": "strong",
                    "emotional_wellbeing": "positive",
                    "care_recommendations": [
                        "Continue current memory exercises",
                        "Maintain social connections",
                        "Regular cognitive monitoring"
                    ],
                    "interventions": [
                        "Memory enhancement activities",
                        "Social engagement programs",
                        "Regular health checkups"
                    ]
                }
            
            return {
                'patient_id': patient_id,
                'summary': summary,
                'interview_data': context,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                'error': str(e),
                'summary': {}
            }
    
    async def get_real_time_feedback(self, current_response: str, question: str) -> Dict:
        """
        Get real-time feedback during interview
        """
        try:
            if GEMINI_AVAILABLE and self.model:
                feedback_prompt = f"""
                Provide real-time feedback for a dementia patient interview.
                
                Current Question: "{question}"
                Patient Response: "{current_response}"
                
                Provide immediate feedback on:
                1. Response quality
                2. Engagement level
                3. Suggested follow-up questions
                4. Any concerns to address
                
                Keep feedback encouraging and supportive.
                """
                
                response = await self._get_gemini_response(feedback_prompt)
                feedback = json.loads(response)
            else:
                # Fallback feedback
                feedback = {
                    "response_quality": "good",
                    "engagement_level": "high",
                    "follow_up_questions": [
                        "Can you tell me more about that?",
                        "How did that make you feel?",
                        "What else do you remember about that time?"
                    ],
                    "concerns": "none",
                    "encouragement": "Excellent response! You're doing great."
                }
            
            return {
                'feedback': feedback,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                'error': str(e),
                'feedback': {}
            }
    
    async def _get_gemini_response(self, prompt: str) -> str:
        """
        Get response from Gemini AI using either google-generativeai or direct HTTP
        """
        try:
            # Try using google-generativeai if available
            if self.model and self.model != "http":
                response = self.model.generate_content(prompt)
                return response.text
            # Try direct HTTP request to Gemini API
            elif self.model == "http" or GEMINI_API_KEY:
                return await self._get_gemini_http_response(prompt)
            else:
                return '{"error": "Gemini AI not available"}'
        except Exception as e:
            print(f"Gemini API error: {str(e)}")
            # Try HTTP fallback
            if GEMINI_API_KEY:
                try:
                    return await self._get_gemini_http_response(prompt)
                except Exception as http_error:
                    print(f"Gemini HTTP API error: {str(http_error)}")
            return '{"error": "Gemini API error"}'
    
    async def _get_gemini_http_response(self, prompt: str) -> str:
        """
        Make direct HTTP request to Gemini API
        """
        try:
            url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent"
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {GEMINI_API_KEY}"
            }
            
            data = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ]
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.post(url, headers=headers, json=data, timeout=30.0)
                
                if response.status_code == 200:
                    result = response.json()
                    if "candidates" in result and len(result["candidates"]) > 0:
                        content = result["candidates"][0]["content"]
                        if "parts" in content and len(content["parts"]) > 0:
                            return content["parts"][0]["text"]
                
                print(f"Gemini HTTP API error: {response.status_code} - {response.text}")
                return '{"error": "Gemini HTTP API error"}'
                
        except Exception as e:
            print(f"Gemini HTTP request error: {str(e)}")
            return '{"error": "Gemini HTTP request error"}'
    
    def get_patient_context(self, patient_id: str) -> Dict:
        """
        Get stored context for a patient
        """
        return self.interview_context.get(patient_id, {})
    
    async def find_relevant_memories(self, current_response: str, patient_id: str, question: str) -> Dict:
        """
        Find existing memories that are relevant to the current response
        """
        try:
            # Import database here to avoid circular imports
            from database import Database, Collections
            
            db = Database.get_db()
            
            # Get all memories for this patient
            memories = await db[Collections.MEMORIES].find(
                {"patient_id": patient_id}
            ).to_list(length=50)
            
            if not memories:
                return {
                    "relevant_memories": [],
                    "suggested_follow_up": "This is your first memory! Keep sharing your stories.",
                    "message": "No previous memories found to compare with."
                }
            
            # Analyze current response for themes
            if GEMINI_AVAILABLE and self.model:
                analysis_prompt = f"""
                Analyze this response for key themes and emotional content:
                
                Question: "{question}"
                Response: "{current_response}"
                
                Extract:
                1. Main themes (family, childhood, emotions, places, people, activities)
                2. Emotional tone (happy, sad, nostalgic, excited, calm)
                3. Key keywords for memory matching
                4. Suggested follow-up questions
                
                Return as JSON with: themes, emotional_tone, keywords, follow_up_questions
                """
                
                response = await self._get_gemini_response(analysis_prompt)
                try:
                    analysis = json.loads(response)
                except:
                    analysis = self._fallback_response_analysis(current_response, question)
            else:
                analysis = self._fallback_response_analysis(current_response, question)
            
            # Find relevant memories based on themes and keywords
            relevant_memories = []
            for memory in memories:
                relevance_score = self._calculate_memory_relevance(
                    memory, analysis, current_response
                )
                
                if relevance_score > 2:  # Lower threshold to include more relevant memories
                    relevant_memories.append({
                        "memory_id": str(memory.get("_id", "")),
                        "memory_title": memory.get("title", "Untitled Memory"),
                        "memory_content": memory.get("content", "")[:200] + "...",
                        "relevance_score": relevance_score,
                        "connection": self._generate_connection_text(memory, analysis),
                        "suggested_prompt": self._generate_suggested_prompt(memory, analysis)
                    })
            
            # Sort by relevance score
            relevant_memories.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            # Limit to top 3 most relevant memories
            relevant_memories = relevant_memories[:3]
            
            # Generate follow-up suggestion
            suggested_follow_up = analysis.get("follow_up_questions", [""])[0] if analysis.get("follow_up_questions") else "Can you tell me more about that?"
            
            return {
                "relevant_memories": relevant_memories,
                "suggested_follow_up": suggested_follow_up,
                "analysis": analysis,
                "message": f"Here are some past memories that may bring a smile to your face! Found {len(relevant_memories)} relevant memories."
            }
            
        except Exception as e:
            print(f"Error finding relevant memories: {str(e)}")
            return {
                "relevant_memories": [],
                "suggested_follow_up": "Can you tell me more about that?",
                "message": "Here are some past memories that may bring a smile to your face!",
                "error": str(e)
            }
    
    def _fallback_response_analysis(self, response: str, question: str) -> Dict:
        """
        Fallback analysis when AI is not available
        """
        # Simple keyword-based analysis
        response_lower = response.lower()
        question_lower = question.lower()
        
        themes = []
        if any(word in response_lower for word in ["family", "mother", "father", "grandma", "grandpa"]):
            themes.append("family")
        if any(word in response_lower for word in ["childhood", "young", "kid", "school"]):
            themes.append("childhood")
        if any(word in response_lower for word in ["happy", "joy", "smile", "laugh"]):
            themes.append("happiness")
        if any(word in response_lower for word in ["home", "house", "kitchen", "garden"]):
            themes.append("home")
        if any(word in response_lower for word in ["food", "dinner", "meal", "cook"]):
            themes.append("food")
        
        emotional_tone = "happy" if any(word in response_lower for word in ["happy", "joy", "love", "wonderful"]) else "neutral"
        
        keywords = [word for word in response_lower.split() if len(word) > 3]
        
        follow_up_questions = [
            "Can you tell me more about that?",
            "How did that make you feel?",
            "What else do you remember about that time?"
        ]
        
        return {
            "themes": themes,
            "emotional_tone": emotional_tone,
            "keywords": keywords[:5],
            "follow_up_questions": follow_up_questions
        }
    
    def _calculate_memory_relevance(self, memory: Dict, analysis: Dict, current_response: str) -> float:
        """
        Calculate how relevant a memory is to the current response
        """
        score = 0.0
        memory_content = memory.get("content", "").lower()
        memory_title = memory.get("title", "").lower()
        
        # Check theme matches
        themes = analysis.get("themes", [])
        for theme in themes:
            if theme in memory_content or theme in memory_title:
                score += 2.0
        
        # Check keyword matches
        keywords = analysis.get("keywords", [])
        for keyword in keywords:
            if keyword in memory_content or keyword in memory_title:
                score += 1.0
        
        # Check emotional tone match
        emotional_tone = analysis.get("emotional_tone", "neutral")
        memory_mood = memory.get("mood", "neutral")
        if emotional_tone == memory_mood:
            score += 1.5
        
        # Check for similar words in current response
        current_words = set(current_response.lower().split())
        memory_words = set(memory_content.split())
        common_words = current_words.intersection(memory_words)
        score += len(common_words) * 0.5
        
        return min(score, 10.0)  # Cap at 10
    
    def _generate_connection_text(self, memory: Dict, analysis: Dict) -> str:
        """
        Generate text explaining the connection between current response and memory
        """
        themes = analysis.get("themes", [])
        memory_title = memory.get("title", "")
        
        if "family" in themes:
            return f"This memory about '{memory_title}' also involves family, just like your current story."
        elif "childhood" in themes:
            return f"This memory about '{memory_title}' is also from your childhood, similar to what you're sharing."
        elif "happiness" in themes:
            return f"This memory about '{memory_title}' also brings joy, just like your current story."
        elif "home" in themes:
            return f"This memory about '{memory_title}' also involves home, similar to your current story."
        else:
            return f"This memory about '{memory_title}' shares similar themes with your current story."
    
    def _generate_suggested_prompt(self, memory: Dict, analysis: Dict) -> str:
        """
        Generate a suggested prompt to help the patient recall more details about the memory
        """
        memory_title = memory.get("title", "")
        themes = analysis.get("themes", [])
        
        if "family" in themes:
            return f"Look back at this memory where you shared about '{memory_title}' - what other family moments come to mind?"
        elif "childhood" in themes:
            return f"Look back at this memory where you shared about '{memory_title}' - what other childhood adventures do you remember?"
        elif "happiness" in themes:
            return f"Look back at this memory where you shared about '{memory_title}' - what other joyful moments can you recall?"
        elif "home" in themes:
            return f"Look back at this memory where you shared about '{memory_title}' - what other home memories are special to you?"
        else:
            return f"Look back at this memory where you shared about '{memory_title}' - what other details can you remember?"

# Initialize the service
interview_analysis_service = InterviewAnalysisService() 